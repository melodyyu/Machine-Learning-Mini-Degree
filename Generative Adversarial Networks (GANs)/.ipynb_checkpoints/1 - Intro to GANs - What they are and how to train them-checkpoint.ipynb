{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definitions: \n",
    "**Discriminative models**: computes the probability that given input belongs to a particular class - p(y|x)\n",
    "- Usually *supervised* learning\n",
    "- Useful for classification (i.e can be used to map an input x to a particular class y) <br>\n",
    "\n",
    "**Generative model**: Learn the joint distribution of the input *and* class labels - p(x,y)\n",
    "- Usually *unsupervised* learning \n",
    "- Convert to p(y|x) using probability rules \n",
    "- Useful for creating new data that looks like training data (i.e images) <br>\n",
    "\n",
    "**Adversarial training**: Two or more \"players\" (i.e game AIs) are competing against each other (ex: Pacman) \n",
    "- What algorithm does it apply: minimax (algorithm that searches for optimized moves assuming that the opponent is also playing optimally) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks (GANs)\n",
    "- An example: counterfeiters and police, to represent the two models. \n",
    "    - Two players: \n",
    "        - counterfeiters to make money that can fool the police == **generator**\n",
    "        - policemen who classify the money as real or fake == **discriminator**\n",
    "    - Their process:\n",
    "        - Analyze the situation using game theory (minimax) to achieve their goal \n",
    "    - End goal:\n",
    "        - Generator creates examples so well that discriminator can't distinguish which are real and fake \n",
    "- Flow:\n",
    "    - Generator takes in noise to produce fake data (that is indistinguishable from the training set) \n",
    "    - Discriminator takes in the fake data and real data\n",
    "    - Discriminator then determines which data is genuine. Ultimately, we want the generator to win."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How we train GANs:\n",
    "- Generator G is a neural network that \"up-samples\" to generate examples\n",
    "    - G receives noise distributed according to a prior $p_{z}$ (usually Gaussian) \n",
    "    - G produces examples in input-space <br>\n",
    "- Discriminator D is a neural network that classifies \n",
    "    - D receives input from input-space, fake or real \n",
    "    - a probability that the input was real (1-that probability = p(fake)) \n",
    "    \n",
    "#### Why are GANs adversarial? \n",
    "- Minimax game between D and G (they're competing)\n",
    "- D is trained to *maximize* the probability of detecting real and fake examples\n",
    "- G is trained to *minimize* the probability \n",
    "- Graphical explanation: if G has its own graph, then D would do its best to match that graph. \n",
    "    \n",
    "#### Detailed explanation:\n",
    "1. Generate batch of fake examples from G using noise prior (ground-truth label is 0)\n",
    "2. Select batch of real examples from training data (ground-truth label is 1) \n",
    "3. Work first with D. Train D on both batches separately\n",
    "    - Think of G as frozen, unable to work with  \n",
    "4. Then, freeze D and train G, using random noise as input and 1 as ground-truth for all examples\n",
    "    - Model for training G generates example from noise input and passes example through frozen D for input\n",
    "        \n",
    "#### Tips and Tricks for training: \n",
    "- Normalize inputs between [-1,1] instead of [0,1]. Use tanh for generator \n",
    "- Sample noise from Gaussian distribution \n",
    "- Avoid sparse gradients\n",
    "    - Prefer leaky ReLUs instead of ReLUs (reasoning: practically identical, but leaky ReLUs have a small negative value when it's <1, which allows it to avoid being sparse) \n",
    "    - Prefer strided convolutions or average pooling instead of max pooling \n",
    "- Use Adam as the optimizer \n",
    "- One-sided label smoothing: use 0 and 0.9 instead of 0 and 1 for discriminator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges with training GANs\n",
    "- G isn't producing diverse outputs\n",
    "    - Known as **Mode Collapse**: G produces low-diversity (also identical) copies of the same image.\n",
    "    - Rather than learning the input distribution of the differences, G only learns the images that fool D and produces similar images with slight differences. \n",
    "    - When this occurs, the minimum and maxium is swapped. (normally G=min and max=D, but mode collapse flips these) \n",
    "- Problems with:\n",
    "    - counting (i.e produces 4 eyes instead of 2 for a dog)\n",
    "    - perspective (i.e an image has perspectives from multiple POVs) \n",
    "    - global structure (i.e nose placed where ears should be) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
