{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Obtain the data\n",
    "- Download MNIST dataset \n",
    "- Examines images and labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist #get mnist from the keras datasets package \n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data() #set into training(60k) and testing images(10k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "#check you have the correct datasets\n",
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#examine how the images look (x=image(the arrays/bytes it's made out of), y=label)\n",
    "#each number is from 0-255, to represent the color (remember:255=black, 0=white)\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show what the image looks, pictorially\n",
    "plt.imshow(x_train[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the label name (indicates what picture is supposed to be)\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the training data to ensure testing and training images are not the same\n",
    "plt.imshow(x_test[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Layers\n",
    "- Convolution layer\n",
    "- Flatten layer\n",
    "- Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the imports (layers and model)\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense \n",
    "from tensorflow.keras import Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The convolutional layer (convolutional layers apply filters to make images smaller). \n",
    "#Filters: # of output images. Kernel: # of weights applied to input.  \n",
    "#Conv2D(filters, kernel_size=, activation='relu')\n",
    "\n",
    "#Flatten the layer\n",
    "#Flatten()\n",
    "\n",
    "#Dense layer - neurons: number of outputs, activation: typically 'relu' or 'softmax'\n",
    "#Dense(neurons, activation=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(Model):\n",
    "    def __init__(self):\n",
    "        #initializer for the model \n",
    "        super(MNISTModel, self).__init__()\n",
    "        \n",
    "        #add in the layers\n",
    "        self.conv1 = Conv2D(32, 3, activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 = Dense(128, activation='relu')\n",
    "        \n",
    "        #condense the outputs into smaller ones. \n",
    "        self.dense2 = Dense(10, activation='softmax')\n",
    "        \n",
    "    #use call notation bc this way, we can simpy write 'Model[]' to use the function \n",
    "    def call(self,x): \n",
    "        #store the input into convolutional 1 and get input of that. feed the output as input into next layer. \n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.flatten(x1)\n",
    "        x3 = self.dense1(x2)\n",
    "        return self.dense2(x3)\n",
    "    \n",
    "model = MNISTModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Loss and Optimizer\n",
    "- Add a loss function\n",
    "- Add an optimizer function \n",
    "- Add a way to measure loss and accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a loss and optimizer function \n",
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy() #will calculate total loss depending on which category current output creates. remember, we want to maximize the possibility that the output is only in one category, so we use sparsecategory which does so\n",
    "optimizer = tf.keras.optimizers.Adam() #optimizer: simply modifies learning rate. We use Adam - modifies learning rate based on how well the training is going (decreases learning rate if it's struggling, increases it if it's easier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify loss and accuracy metrics for training \n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss') #gets avg of all training loss \n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify loss and accuracy metrics for testing \n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss') #gets avg of all testing loss \n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy') #sees if output falls in one category as desired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Train and Test Step \n",
    "- Add the function to run when training the model\n",
    "- Add the function to run when testing  the model (diff is that we don't modify the weights here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training step\n",
    "@tf.function\n",
    "#input = images, outputs = labels (outputs are correct answers for inputs) \n",
    "def train_step(inputs, outputs):\n",
    "    with tf.GradientTape() as tape: #used to apply gradient and change weights/biases  \n",
    "        predictions = model(inputs) #get model's outputs(predictions) based on current weights/biases \n",
    "        loss = loss_function(outputs, predictions) #get the loss based on what the model is outputting and what the actual output is, so that there's always a difference between what the model thinks is a correct answer and the actual correct answer \n",
    "    gradients = tape.gradient(loss, model.trainable_variables) #getting changes we need to make - trainable_variables are all the possible weights/biases of each layer \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables)) #making those changes to apply those weights to the trainable_variables so the loss is lower \n",
    "    \n",
    "    #keep track of current loss and accuracy\n",
    "    train_loss(loss) #should decrease over time \n",
    "    train_accuracy(outputs, predictions) #get the actual correct answer and the output's answer to see if model is correct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing tape\n",
    "@tf.function\n",
    "def test_step(inputs, outputs):\n",
    "    with tf.GradientTape() as tape: \n",
    "        predictions = model(inputs) \n",
    "        loss = loss_function(outputs, predictions) \n",
    "  \n",
    "    test_loss(loss) \n",
    "    test_accuracy(outputs, predictions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - Formatting Data\n",
    "- Format our inputs \n",
    "- Format our outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "x_train = x_train[..., tf.newaxis] #turns all columns in each row to be its own array. the '...' grabs all values. makes it easier for model to compute  \n",
    "x_test = x_test[..., tf.newaxis] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the training data, slice them up, shuffle them 10,000 at a time (helps to eliminate false positives) and have them as input batches of 32 \n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)\n",
    "\n",
    "#divide test data into batches as well  \n",
    "test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6 - Training \n",
    "- Write a train loop \n",
    "- Train and evaluate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer mnist_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epochs: 1, Train loss: 0.13236308097839355, Train accuracy: 0.9598666429519653, Test loss: 0.05895889922976494 Test accuracy: 0.9811000227928162\n",
      "Epochs: 2, Train loss: 0.04054347053170204, Train accuracy: 0.9876999855041504, Test loss: 0.05095290020108223 Test accuracy: 0.9828000068664551\n",
      "Epochs: 3, Train loss: 0.021493345499038696, Train accuracy: 0.9929999709129333, Test loss: 0.058491699397563934 Test accuracy: 0.9818000197410583\n",
      "Epochs: 4, Train loss: 0.013767946511507034, Train accuracy: 0.9955333471298218, Test loss: 0.05577279254794121 Test accuracy: 0.9843000173568726\n",
      "Epochs: 5, Train loss: 0.01016322337090969, Train accuracy: 0.9965000152587891, Test loss: 0.05706515908241272 Test accuracy: 0.9850999712944031\n"
     ]
    }
   ],
   "source": [
    "epochs = 5 \n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #run train step on train data\n",
    "    for train_inputs, train_labels in train_data:\n",
    "        train_step(train_inputs, train_labels)\n",
    "        \n",
    "    #run test step on test data \n",
    "    for test_inputs, test_labels in test_data:\n",
    "        test_step(test_inputs, test_labels)\n",
    "        \n",
    "    #print results for each epoch \n",
    "    template = 'Epochs: {}, Train loss: {}, Train accuracy: {}, Test loss: {} Test accuracy: {}'\n",
    "    print(template.format(\n",
    "        epoch + 1, \n",
    "        train_loss.result(), \n",
    "        train_accuracy.result(), \n",
    "        test_loss.result(), \n",
    "        test_accuracy.result()\n",
    "    ))\n",
    "    \n",
    "    #reset training and test loss and accuracy bc we don't want the previous states to skew results\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 7 - Analyze results\n",
    "We see that after five epochs, our accuracy is 99.7%, which is pretty good. Test loss increases a bit, which is not a big concern. Our accuracy also increased, from 98.2% to 98.4%, which indicate that our model was successful in its training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
