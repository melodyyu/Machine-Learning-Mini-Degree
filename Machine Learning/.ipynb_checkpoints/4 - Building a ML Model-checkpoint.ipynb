{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to building a ML model\n",
    "1. Figure out whether we can use ML to solve the problem and if so, what model to use \n",
    "2. Gather and format the data \n",
    "3. Build the neural network itself \n",
    "4. Train the model \n",
    "5. Test and evaluate the model\n",
    "6. Refactor and gather more data if needed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figuring out what model to use \n",
    "- There could be several models that are able to solve your problem, but how do you determine the optimal model?\n",
    "    - Try different ones, see which works best\n",
    "    - Research ohther people's, see how similar problems have been solved "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Data \n",
    "- The more data the better \n",
    "- Varied data is better so the model can see different kinds of input output combos \n",
    "- There are many pre-built datasets online (some for free, some pre-formatted) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting Data\n",
    "- Input and output data should be of the same format \n",
    "- Assign labels/expected outputs to each data point \n",
    "- Divide the data into training and testing, usually with an 80/20 split (make sure there's more training than testing)\n",
    "- Make sure there's no overlap between training and testing data sets (or else the model will have a false confidence correctness level) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model\n",
    "- Start with an input layer: generally one node for each point of input (ex: 10x10 picture, one node for each pixel = 100 nodes)\n",
    "- Number and structure of the hidden layers depends on the type of model you are building \n",
    "- Output layer: needs one node for each point of output (ex: 10x10 picture, 100 nodes total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "Model adjusts node weights in order to get closer to expected output\n",
    "- With supervised learning, we feed inputs and expected outputs into model. Model produces an output for each input given its curent weights and biases. \n",
    "- Difference between expected and actual output is the \"error\" or \"loss\" function. Our overall goal is to minimize this, as it means the actual output is closer to the expected output (the smaller the loss/error, the closer the sum)\n",
    "- With perceptrons, this is done through backpropagation using a gradient descent optimizer\n",
    "- Usually run through the data multiple times (each iteration is called an \"epoch\"). Run through batches rather than all the data at once\n",
    "- Ends at user's discretion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model \n",
    "- Feeding in testing data, but not adjusting weights\n",
    "- Goal: see hwo well model performs in real world with some test data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactoring the model\n",
    "- After running through testing, we'll have a loss and percent accuracy. \n",
    "- We modify if the loss is sufficiently low and the accuracy is sufficiently high. If not, we need to modify the structure or get more data\n",
    "- Possible issues: not enough data and need more, layers need to be changed, try a different model type"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
