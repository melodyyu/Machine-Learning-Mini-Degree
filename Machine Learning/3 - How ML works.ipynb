{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syllabus:\n",
    "- Definitions\n",
    "- Common ML structures (types of networks, how we structure them)\n",
    "- Build a complete ML program (specifically, a supervised learning structure) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitions:\n",
    " - Neural network: often called a computational graph, a network of interconnected nodes through which data flows. Begins with input nodes, has some number of intermediate nodes (known as \"hidden\" nodes, that modify the value somehow) and ends with output nodes \n",
    " - Node: often called a neuron. point in neural network that which data flow, typically has weights, biases and an activation function. Input and ouptut nodes are simply entry/exit points for data. Note: every single node is connected to every single other node in the other layers. <br>\n",
    " (Node specific)\n",
    " - Weight: Factor by which input is multiplied. Each node input has a weight. Purpose of training is to modify weight values. \n",
    " - Bias: Number added to product of weights and inputs. Proivdes a way to ensure neurons are passing on outputs.\n",
    " - Weighted sum: Multiply each input by the expected weight, add the bias to each and sum them up. \n",
    " \n",
    " - Activation function: Function applied to weighted sum to transform output to a value that indicates whether or not a neuron fies or passes on its value. Two types:\n",
    "  1. ReLu: replace Sigmoid and TanH functions if it solves the vanishing gradient problem (values approaching 0 == dead neurons). If input < 0: output is 0. If input > 0, output is unmodified input \n",
    "      - TanH function: inputs produce output between -1 and 1 \n",
    "      - Sigmoid function: inputs produce output between 0 and 1 \n",
    "      - ReLu function: inputs produce output between 0 and input \n",
    "      **very low values go to zero and high values are themselves \n",
    "  2. Softmax: typically used in final layer of probability and classification models \n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Machine Learning Structures\n",
    "(1) Single Layer Feed Forward\n",
    "- inputs are modified through an activation function, which are fed directly to output nodes\n",
    "- often called \"perceptrons\"\n",
    "- most common algorithm used is \"delta rule\" = calculate difference between difference and actual output and adjust weights in order to minimize differences. \n",
    "- exclusively supervised learning (we're telling the model what the answer should be and telling them they should reach that answer) \n",
    "- \"single layer\" refers to the singular hidden layer\n",
    "- Linear gradient descent optimizer\n",
    "\n",
    "(2) Multiple Layer Feed Foward\n",
    "- similar to above, there are multiple hidden layers instead. Note that every node is connected to every node in the next layer\n",
    "- most common algorithm used is \"back-propagation\" = compares actual and expected outpus and produces an error based on the total differences. then adjusts weights and reruns during training \n",
    "- Non-linear gradient descent optimizer \n",
    "- Good for solving complex problems, but much slower because there are so many more connections <br>\n",
    "--- (only examples of 1 + 2 will be covered) \n",
    "\n",
    "(3) Radial Basis Function (RBF) \n",
    "- Similar to perceptron, but has a hidden layer with neurons with radial basis activation functions (instead of the typical ReLu). These activation functions fire maximally with distance between weights are similar to inputs\n",
    "- Good for detecting anomalies and classification problems. Not for extrapolation\n",
    "- Supervised learning\n",
    "\n",
    "(4) Convolutional Neural Network \n",
    "- similar to perceptron, but has hidden layer(s) with convolution functions and pooling functions (=/= activation functions) \n",
    "- Convolution functions create a complex pattern by processing smaller, less complex units\n",
    "- Good for image recognition and classification\n",
    "- Their problem: overfitting == deducing patterns when there are none and lose sight of the actual pattern they're trying to find --> gives us a false sense of confidence in the model \n",
    "- Supervised learning \n",
    "\n",
    "(5) Recurrent Neural Network \n",
    "- No longer feed-forward (input to output). Has hidden layers that generally contain LSTM cells (retains memory/state and output is dependent on the current input,state) \n",
    "- Similar to a multi-layer perceptron, \n",
    "- Good for text/speech-related problems (ex: speech recognition, language translation) \n",
    "- Supervised or reinforcement learning (depends on the problem we're tring to solve) \n",
    "\n",
    "(6) Modular Neural Network\n",
    "- A neural network separated into two or more independent modules and managed by an intermediary\n",
    "- Modules process input separately and do not interact, usually perform a specific task. Intermediry takes each module's outputs and puts them together without modifying them \n",
    "- Less prone to failure because it can isolate each case \n",
    "\n",
    "(7) Sequence to Sequence Models \n",
    "- three part networks: encoder, intermediary and decoder \n",
    "    Encoder: encodes all input in some format, assigns value to each possible part of an input\n",
    "    Decoder: encodes output to make them readable \n",
    "- Good at producing different kind of input from output or when working with outputs of different lengths (i.e language translation, image captioning) \n",
    "- Supervised or reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
